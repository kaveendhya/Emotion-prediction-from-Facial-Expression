{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Video For Happy Emotions"
      ],
      "metadata": {
        "id": "_CKhsDWewqV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Initialize the face cascade classifier\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Initialize the emotion labels\n",
        "emotion_labels = ['HAPPY', 'SAD', 'NEUTRAL', 'FEAR', 'DISGUST', 'ANGRY']\n",
        "\n",
        "# Load your pre-trained emotion prediction model (replace 'your_emotion_model.h5' with the actual path)\n",
        "def load_emotion_model(model_path):\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    return model\n",
        "\n",
        "# Function to preprocess a single frame\n",
        "def preprocess_frame(frame):\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    resized_frame = cv2.resize(gray_frame, (48, 48))\n",
        "    normalized_frame = resized_frame / 255.0\n",
        "    preprocessed_frame = np.expand_dims(normalized_frame, axis=-1)\n",
        "    return preprocessed_frame\n",
        "\n",
        "# Function to make predictions on a single frame\n",
        "def predict_emotion(frame, emotion_model):\n",
        "    preprocessed_frame = preprocess_frame(frame)\n",
        "    emotions = emotion_model.predict(np.array([preprocessed_frame]))\n",
        "    return emotions[0]\n",
        "\n",
        "# Function to detect faces in a frame\n",
        "def detect_faces(frame):\n",
        "    # Convert the frame to grayscale\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    # Convert the (x, y, w, h) rectangles to tuples\n",
        "    face_rectangles = [(x, y, x + w, y + h) for (x, y, w, h) in faces]\n",
        "\n",
        "    return face_rectangles\n",
        "\n",
        "# Function to calculate the average emotion percentages\n",
        "def calculate_average_emotion(frame_emotions):\n",
        "    total_emotions = np.sum(frame_emotions, axis=0)\n",
        "    total_sum = np.sum(total_emotions)\n",
        "\n",
        "    # Handle the case where total_sum is zero (no emotion predictions made)\n",
        "    if total_sum == 0:\n",
        "        return np.zeros(len(emotion_labels))\n",
        "\n",
        "    average_emotions = (total_emotions / total_sum) * 100\n",
        "    return average_emotions\n",
        "\n",
        "# Load the emotion prediction model\n",
        "model_path = '/content/drive/MyDrive/Reserch/models/best_model_cifar10_female.hdf5'   # Replace with your model path\n",
        "emotion_model = load_emotion_model(model_path)\n",
        "\n",
        "# Open the video file\n",
        "video_path = '/content/drive/MyDrive/Reserch/video/IMG_7764.MOV'  # Replace with your video path\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Initialize an empty list to store emotion percentages for each frame\n",
        "frame_emotions = []\n",
        "\n",
        "# Frame extraction rate\n",
        "frame_extraction_rate = 3  # Extract a frame every 3 frames (adjust as needed)\n",
        "\n",
        "frame_counter = 0\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame_counter += 1\n",
        "\n",
        "    if frame_counter % frame_extraction_rate == 0:\n",
        "        # Detect faces in the frame\n",
        "        detected_faces = detect_faces(frame)\n",
        "\n",
        "        for (x, y, x2, y2) in detected_faces:\n",
        "            face = frame[y:y2, x:x2]\n",
        "\n",
        "            # Make predictions for the current face\n",
        "            emotions = predict_emotion(face, emotion_model)\n",
        "\n",
        "            # Append the emotion percentages to the list\n",
        "            frame_emotions.append(emotions)\n",
        "\n",
        "# Release the video capture object\n",
        "cap.release()\n",
        "\n",
        "# Calculate the average emotion percentages over all frames\n",
        "average_emotions = calculate_average_emotion(frame_emotions)\n",
        "\n",
        "print(\"Average emotion predictions:\")\n",
        "\n",
        "# Print the average emotion percentages\n",
        "for label, percentage in zip(emotion_labels, average_emotions):\n",
        "    print(f\"{label}: {percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY3HyipQr4il",
        "outputId": "7cb45933-d651-4980-fbc5-7b92a31fead6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "Average emotion predictions:\n",
            "HAPPY: 75.39%\n",
            "SAD: 4.86%\n",
            "NEUTRAL: 3.20%\n",
            "FEAR: 5.41%\n",
            "DISGUST: 0.76%\n",
            "ANGRY: 10.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Initialize the face cascade classifier\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Initialize the emotion labels\n",
        "emotion_labels = ['HAPPY', 'SAD', 'NEUTRAL', 'FEAR', 'DISGUST', 'ANGRY']\n",
        "\n",
        "# Load your pre-trained emotion prediction model (replace 'your_emotion_model.h5' with the actual path)\n",
        "def load_emotion_model(model_path):\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    return model\n",
        "\n",
        "# Function to preprocess a single frame\n",
        "def preprocess_frame(frame):\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    resized_frame = cv2.resize(gray_frame, (48, 48))\n",
        "    normalized_frame = resized_frame / 255.0\n",
        "    preprocessed_frame = np.expand_dims(normalized_frame, axis=-1)\n",
        "    return preprocessed_frame\n",
        "\n",
        "# Function to make predictions on a single frame\n",
        "def predict_emotion(frame, emotion_model):\n",
        "    preprocessed_frame = preprocess_frame(frame)\n",
        "    emotions = emotion_model.predict(np.array([preprocessed_frame]))\n",
        "    return emotions[0]\n",
        "\n",
        "# Function to detect faces in a frame\n",
        "def detect_faces(frame):\n",
        "    # Convert the frame to grayscale\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    # Convert the (x, y, w, h) rectangles to tuples\n",
        "    face_rectangles = [(x, y, x + w, y + h) for (x, y, w, h) in faces]\n",
        "\n",
        "    return face_rectangles\n",
        "\n",
        "# Function to calculate the average emotion percentages\n",
        "def calculate_average_emotion(frame_emotions):\n",
        "    total_emotions = np.sum(frame_emotions, axis=0)\n",
        "    total_sum = np.sum(total_emotions)\n",
        "\n",
        "    # Handle the case where total_sum is zero (no emotion predictions made)\n",
        "    if total_sum == 0:\n",
        "        return np.zeros(len(emotion_labels))\n",
        "\n",
        "    average_emotions = (total_emotions / total_sum) * 100\n",
        "    return average_emotions\n",
        "\n",
        "# Load the emotion prediction model\n",
        "model_path = '/content/drive/MyDrive/Reserch/models/best_model_cifar10_female.hdf5'  # Replace with your model path\n",
        "emotion_model = load_emotion_model(model_path)\n",
        "\n",
        "# Open the video file\n",
        "video_path = '/content/drive/MyDrive/Reserch/video/IMG_7755.MOV'  # Replace with your video path\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Initialize an empty list to store emotion percentages for each frame\n",
        "frame_emotions = []\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    detected_faces = detect_faces(frame)\n",
        "\n",
        "    for (x, y, x2, y2) in detected_faces:\n",
        "        face = frame[y:y2, x:x2]\n",
        "\n",
        "        # Make predictions for the current face\n",
        "        emotions = predict_emotion(face, emotion_model)\n",
        "\n",
        "\n",
        "        # Append the emotion percentages to the list\n",
        "        frame_emotions.append(emotions)\n",
        "\n",
        "# Release the video capture object\n",
        "cap.release()\n",
        "\n",
        "# Calculate the average emotion percentages over all frames\n",
        "average_emotions = calculate_average_emotion(frame_emotions)\n",
        "\n",
        "print(\"average prediction\")\n",
        "\n",
        "# Print the average emotion percentages\n",
        "for label, percentage in zip(emotion_labels, average_emotions):\n",
        "    print(f\"{label}: {percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVA95-GkDoeq",
        "outputId": "12c77753-a1a0-445b-aaa8-e4849a05d2f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 993ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "average prediction\n",
            "HAPPY: 57.50%\n",
            "SAD: 26.83%\n",
            "NEUTRAL: 7.40%\n",
            "FEAR: 5.62%\n",
            "DISGUST: 0.81%\n",
            "ANGRY: 1.84%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Initialize the face cascade classifier\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Initialize the emotion labels\n",
        "emotion_labels = ['HAPPY', 'SAD', 'NEUTRAL', 'FEAR', 'DISGUST', 'ANGRY']\n",
        "\n",
        "# Load your pre-trained emotion prediction model (replace 'your_emotion_model.h5' with the actual path)\n",
        "def load_emotion_model(model_path):\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    return model\n",
        "\n",
        "# Function to preprocess a single frame\n",
        "def preprocess_frame(frame):\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    resized_frame = cv2.resize(gray_frame, (48, 48))\n",
        "    normalized_frame = resized_frame / 255.0\n",
        "    preprocessed_frame = np.expand_dims(normalized_frame, axis=-1)\n",
        "    return preprocessed_frame\n",
        "\n",
        "# Function to make predictions on a single frame\n",
        "def predict_emotion(frame, emotion_model):\n",
        "    preprocessed_frame = preprocess_frame(frame)\n",
        "    emotions = emotion_model.predict(np.array([preprocessed_frame]))\n",
        "    return emotions[0]\n",
        "\n",
        "# Function to detect faces in a frame\n",
        "def detect_faces(frame):\n",
        "    # Convert the frame to grayscale\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    # Convert the (x, y, w, h) rectangles to tuples\n",
        "    face_rectangles = [(x, y, x + w, y + h) for (x, y, w, h) in faces]\n",
        "\n",
        "    return face_rectangles\n",
        "\n",
        "# Function to calculate the average emotion percentages\n",
        "def calculate_average_emotion(frame_emotions):\n",
        "    total_emotions = np.sum(frame_emotions, axis=0)\n",
        "    total_sum = np.sum(total_emotions)\n",
        "    average_emotions = (total_emotions / total_sum) * 100\n",
        "    return average_emotions\n",
        "\n",
        "# Load the emotion prediction model\n",
        "model_path = '/content/drive/MyDrive/Reserch/models/best_model_cifar10_female.hdf5'  # Replace with your model path\n",
        "emotion_model = load_emotion_model(model_path)\n",
        "\n",
        "# Open the video file\n",
        "video_path = '/content/drive/MyDrive/Reserch/video/IMG_7764.MOV'  # Replace with your video path\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get the total number of frames and frames per second (fps)\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Calculate the video duration in seconds\n",
        "video_duration = total_frames / fps\n",
        "\n",
        "# Determine the frame extraction rate based on video duration\n",
        "if video_duration <= 15:\n",
        "    frame_extraction_rate = 10\n",
        "elif video_duration <= 30:\n",
        "    frame_extraction_rate = 20\n",
        "else:\n",
        "    frame_extraction_rate = 25\n",
        "\n",
        "frame_counter = 0\n",
        "\n",
        "# Initialize an empty list to store emotion percentages for each frame\n",
        "frame_emotions = []\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame_counter += 1\n",
        "\n",
        "    if frame_counter % frame_extraction_rate == 0:\n",
        "        # Detect faces in the frame\n",
        "        detected_faces = detect_faces(frame)\n",
        "\n",
        "        for (x, y, x2, y2) in detected_faces:\n",
        "            face = frame[y:y2, x:x2]\n",
        "\n",
        "            # Make predictions for the current face\n",
        "            emotions = predict_emotion(face, emotion_model)\n",
        "\n",
        "            # Append the emotion percentages to the list\n",
        "            frame_emotions.append(emotions)\n",
        "\n",
        "# Release the video capture object\n",
        "cap.release()\n",
        "\n",
        "# Calculate the average emotion percentages over all frames\n",
        "average_emotions = calculate_average_emotion(frame_emotions)\n",
        "\n",
        "print(\"average prediction\")\n",
        "\n",
        "# Print the average emotion percentages\n",
        "for label, percentage in zip(emotion_labels, average_emotions):\n",
        "    print(f\"{label}: {percentage:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3AAP0osHW5l",
        "outputId": "99ec71dd-ee8f-49b4-c9dd-5a2fbec1682a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 235ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "average prediction\n",
            "HAPPY: 73.66%\n",
            "SAD: 6.25%\n",
            "NEUTRAL: 1.67%\n",
            "FEAR: 6.21%\n",
            "DISGUST: 0.37%\n",
            "ANGRY: 11.84%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Initialize the face cascade classifier\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Initialize the emotion labels\n",
        "emotion_labels = ['HAPPY', 'SAD', 'NEUTRAL', 'FEAR', 'DISGUST', 'ANGRY']\n",
        "\n",
        "# Load your pre-trained emotion prediction model (replace 'your_emotion_model.h5' with the actual path)\n",
        "def load_emotion_model(model_path):\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    return model\n",
        "\n",
        "# Function to preprocess a single frame\n",
        "def preprocess_frame(frame):\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    resized_frame = cv2.resize(gray_frame, (48, 48))\n",
        "    normalized_frame = resized_frame / 255.0\n",
        "    preprocessed_frame = np.expand_dims(normalized_frame, axis=-1)\n",
        "    return preprocessed_frame\n",
        "\n",
        "# Function to make predictions on a single frame\n",
        "def predict_emotion(frame, emotion_model):\n",
        "    preprocessed_frame = preprocess_frame(frame)\n",
        "    emotions = emotion_model.predict(np.array([preprocessed_frame]))\n",
        "    return emotions[0]\n",
        "\n",
        "# Function to detect faces in a frame\n",
        "def detect_faces(frame):\n",
        "    # Convert the frame to grayscale\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    # Convert the (x, y, w, h) rectangles to tuples\n",
        "    face_rectangles = [(x, y, x + w, y + h) for (x, y, w, h) in faces]\n",
        "\n",
        "    return face_rectangles\n",
        "\n",
        "# Function to calculate the average emotion percentages\n",
        "def calculate_average_emotion(frame_emotions):\n",
        "    total_emotions = np.sum(frame_emotions, axis=0)\n",
        "    total_sum = np.sum(total_emotions)\n",
        "    average_emotions = (total_emotions / total_sum) * 100\n",
        "    return average_emotions\n",
        "\n",
        "# Load the emotion prediction model\n",
        "model_path = '/content/drive/MyDrive/Reserch/models/best_model_cifar10_female.hdf5'  # Replace with your model path\n",
        "emotion_model = load_emotion_model(model_path)\n",
        "\n",
        "# Open the video file\n",
        "video_path = '/content/drive/MyDrive/Reserch/video/video6131695912512129686.mp4'  # Replace with your video path\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Initialize an empty list to store emotion percentages for each frame\n",
        "# Initialize an empty list to store emotion percentages for each frame\n",
        "frame_emotions = []\n",
        "\n",
        "# Frame extraction rate\n",
        "frame_extraction_rate = 3  # Extract a frame every 10 frames (adjust as needed)\n",
        "\n",
        "frame_counter = 0\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame_counter += 1\n",
        "\n",
        "    if frame_counter % frame_extraction_rate == 0:\n",
        "        # Detect faces in the frame\n",
        "        detected_faces = detect_faces(frame)\n",
        "\n",
        "        for (x, y, x2, y2) in detected_faces:\n",
        "            face = frame[y:y2, x:x2]\n",
        "\n",
        "            # Make predictions for the current face\n",
        "            emotions = predict_emotion(face, emotion_model)\n",
        "\n",
        "            # Append the emotion percentages to the list\n",
        "            frame_emotions.append(emotions)\n",
        "\n",
        "\n",
        "# Release the video capture object\n",
        "cap.release()\n",
        "\n",
        "# Calculate the average emotion percentages over all frames\n",
        "average_emotions = calculate_average_emotion(frame_emotions)\n",
        "\n",
        "print(\"average prediction\")\n",
        "\n",
        "# Print the average emotion percentages\n",
        "for label, percentage in zip(emotion_labels, average_emotions):\n",
        "    print(f\"{label}: {percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gag5hDv8OmhH",
        "outputId": "3c16bd32-25d9-4a31-d2d3-b56b5f07b3b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "average prediction\n",
            "HAPPY: 7.28%\n",
            "SAD: 35.67%\n",
            "NEUTRAL: 8.50%\n",
            "FEAR: 6.11%\n",
            "DISGUST: 4.98%\n",
            "ANGRY: 37.45%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Video for Fear Emotion"
      ],
      "metadata": {
        "id": "2b1z1C8Yw_7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Initialize the face cascade classifier\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Initialize the emotion labels\n",
        "emotion_labels = ['HAPPY', 'SAD', 'NEUTRAL', 'FEAR', 'DISGUST', 'ANGRY']\n",
        "\n",
        "# Load your pre-trained emotion prediction model (replace 'your_emotion_model.h5' with the actual path)\n",
        "def load_emotion_model(model_path):\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    return model\n",
        "\n",
        "# Function to preprocess a single frame\n",
        "def preprocess_frame(frame):\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    resized_frame = cv2.resize(gray_frame, (48, 48))\n",
        "    normalized_frame = resized_frame / 255.0\n",
        "    preprocessed_frame = np.expand_dims(normalized_frame, axis=-1)\n",
        "    return preprocessed_frame\n",
        "\n",
        "# Function to make predictions on a single frame\n",
        "def predict_emotion(frame, emotion_model):\n",
        "    preprocessed_frame = preprocess_frame(frame)\n",
        "    emotions = emotion_model.predict(np.array([preprocessed_frame]))\n",
        "    return emotions[0]\n",
        "\n",
        "# Function to detect faces in a frame\n",
        "def detect_faces(frame):\n",
        "    # Convert the frame to grayscale\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    # Convert the (x, y, w, h) rectangles to tuples\n",
        "    face_rectangles = [(x, y, x + w, y + h) for (x, y, w, h) in faces]\n",
        "\n",
        "    return face_rectangles\n",
        "\n",
        "# Function to calculate the average emotion percentages\n",
        "def calculate_average_emotion(frame_emotions):\n",
        "    total_emotions = np.sum(frame_emotions, axis=0)\n",
        "    total_sum = np.sum(total_emotions)\n",
        "    average_emotions = (total_emotions / total_sum) * 100\n",
        "    return average_emotions\n",
        "\n",
        "# Load the emotion prediction model\n",
        "model_path = '/content/drive/MyDrive/Reserch/models/best_model_cifar10_female.hdf5'  # Replace with your model path\n",
        "emotion_model = load_emotion_model(model_path)\n",
        "\n",
        "# Open the video file\n",
        "video_path = '/content/drive/MyDrive/Reserch/video/IMG_7768.MOV'  # Replace with your video path\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Initialize an empty list to store emotion percentages for each frame\n",
        "# Initialize an empty list to store emotion percentages for each frame\n",
        "frame_emotions = []\n",
        "\n",
        "# Frame extraction rate\n",
        "frame_extraction_rate = 2  # Extract a frame every 10 frames (adjust as needed)\n",
        "\n",
        "frame_counter = 0\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame_counter += 1\n",
        "\n",
        "    if frame_counter % frame_extraction_rate == 0:\n",
        "        # Detect faces in the frame\n",
        "        detected_faces = detect_faces(frame)\n",
        "\n",
        "        for (x, y, x2, y2) in detected_faces:\n",
        "            face = frame[y:y2, x:x2]\n",
        "\n",
        "            # Make predictions for the current face\n",
        "            emotions = predict_emotion(face, emotion_model)\n",
        "\n",
        "            # Append the emotion percentages to the list\n",
        "            frame_emotions.append(emotions)\n",
        "\n",
        "\n",
        "# Release the video capture object\n",
        "cap.release()\n",
        "\n",
        "# Calculate the average emotion percentages over all frames\n",
        "average_emotions = calculate_average_emotion(frame_emotions)\n",
        "\n",
        "print(\"average prediction\")\n",
        "\n",
        "# Print the average emotion percentages\n",
        "for label, percentage in zip(emotion_labels, average_emotions):\n",
        "    print(f\"{label}: {percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8UDEt3X-oEY",
        "outputId": "ddc415d8-70b5-4994-e5f8-5dd7ccb8cf8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 108ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "average prediction\n",
            "HAPPY: 4.97%\n",
            "SAD: 28.02%\n",
            "NEUTRAL: 5.97%\n",
            "FEAR: 53.77%\n",
            "DISGUST: 0.24%\n",
            "ANGRY: 7.03%\n"
          ]
        }
      ]
    }
  ]
}